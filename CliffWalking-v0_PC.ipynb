{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>INDICATIONS SUR LES JEU</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>ELEMENTS</h5>\n",
    "\n",
    "- bleu: passager\n",
    "- fuchsia: déstination\n",
    "- jaune: taxi vide\n",
    "- vert: taxi + passager\n",
    "- autres lettres: lieux divers\n",
    "\n",
    "\n",
    "<h5>ACTIONS</h5>\n",
    "\n",
    "- 0: déplacer vers le Sud (bas)\n",
    "- 1: déplacer vers le Nord (haut)\n",
    "- 2: déplacer vers l'Est (gauche) \n",
    "- 3: déplacer vers le Ouest (droite) \n",
    "- 4: prendre passager\n",
    "- 5: déposer passager\n",
    "\n",
    "\n",
    "<h5>RECOMPENSES</h5> \n",
    "\n",
    "- DONE (fin de jeu) : 20\n",
    "\n",
    "- Déposer le passager à la mauvaise destination : -10\n",
    "\n",
    "- Prendre un autre passager que le passager bleu : -10\n",
    "\n",
    "- Pour toute autre action, parmi les actions de 0 -> 6 : recompense -1\n",
    "\n",
    "\n",
    "<h5>REGLES</h5>\n",
    "\n",
    "- Le taxi se déplace selon les actions indiquées.\n",
    "\n",
    "- On ne peut pas se déplacer à travers un mur.\n",
    "\n",
    "- Le jeu est fini quand le passager bleu est déposé dans la déstination fuchsia.\n",
    "\n",
    "Lien du jeu : https://www.gymlibrary.ml/environments/toy_text/taxi/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>INDICATION CODE AVEC GYM et NUMPY</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Choisir une nouvelle action de manière aléatoire : \n",
    "\n",
    "    `action = env.action_space.sample()`\n",
    "    \n",
    "Choisir la meilleure action pour un état donné `s`: \n",
    "\n",
    "    Choisir la plus grande valeur pour `Q[s]`\n",
    "\n",
    "Prendre une action avec gym :\n",
    "\n",
    "    `obs, reward, done, info = env.step(action)`\n",
    " \n",
    " `obs`: nouvel état, `reward`: recompense obtenue, `done`: boolean, indiquant si le jeu est fini, `info`, diverses infos sur l'action\n",
    "\n",
    "Choisir la plus grande valeur d'un tableau T: \n",
    "\n",
    "    `max_val = np.argmax(T)`\n",
    "    \n",
    "Dans le tableau Q, la valeur de l'action pour l'état \"s\" et l'action \"a\", est un nombre, et se trouve dans la celulle : \n",
    "\n",
    "`Q[s,a]` ou bien `Q[s][a]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\pierr\\onedrive\\bureau\\oobabooga_windows\\installer_files\\conda\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\pierr\\onedrive\\bureau\\oobabooga_windows\\installer_files\\conda\\lib\\site-packages (from gym) (1.24.3)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\pierr\\onedrive\\bureau\\oobabooga_windows\\installer_files\\conda\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\pierr\\onedrive\\bureau\\oobabooga_windows\\installer_files\\conda\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: pygame in c:\\users\\pierr\\appdata\\roaming\\python\\python310\\site-packages (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonctions epsilon greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui choisit une action avec la méthode epsilon greedy \n",
    "def get_greedy_action(s,epsilon):\n",
    "    # On choisit de manière aléatoire un nombre entre 0 et 1\n",
    "    random_num = np.random.uniform(0, 1) \n",
    "    if random_num < epsilon: # exploration : on choisit une action aléatoire\n",
    "        action_suivante = env.action_space.sample() # Choix aléatoire de la prochaine action     \n",
    "        \n",
    "    else: # exploitation : On choisit la meilleure action pour l'état présent : etat_courant\n",
    "        action_suivante = np.argmax(Q[s]) # Choix de la meilleure action dans la table Q \n",
    "    \n",
    "    return int(action_suivante)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 4\n"
     ]
    }
   ],
   "source": [
    "game = \"CliffWalking-v0\"\n",
    "env = gym.make(game)\n",
    "\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "Q = np.zeros([NUM_STATES, NUM_ACTIONS])\n",
    "\n",
    "gamma =  0.99 # facteur d'actualisation (discount factor)\n",
    "alpha = 0.1 # taux d'apprentissage (learning rate)\n",
    "epsilon = 0.0001 # pour epsilon greedy (1e-3 donne un sous apprentissage)\n",
    "\n",
    "\n",
    "print(NUM_STATES, NUM_ACTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formation de l'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debut de l'épisode: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierr\\OneDrive\\Bureau\\oobabooga_windows\\installer_files\\conda\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.55251899 -10.48036802 -10.4895601  -10.59636051]\n",
      " [-10.04073164 -10.02216103 -10.13072512 -10.04673389]\n",
      " [ -9.50766828  -9.43589669  -9.44632254  -9.53245839]\n",
      " [ -8.80991218  -8.77617864  -8.79712151  -8.81498525]\n",
      " [ -8.10718417  -8.08496883  -8.16116968  -8.11831591]\n",
      " [ -7.41478647  -7.36177403  -7.37208854  -7.44265676]\n",
      " [ -6.67807237  -6.61524253  -6.68422639  -6.65686888]\n",
      " [ -5.96189062  -5.85652813  -5.91265574  -5.94828573]\n",
      " [ -5.24830542  -5.0925415   -5.12925065  -5.21325945]\n",
      " [ -4.32568265  -4.32438097  -4.32184158  -4.45731683]\n",
      " [ -3.58274823  -3.58559021  -3.58754255  -3.58841477]\n",
      " [ -3.01286037  -2.8519746   -2.86002783  -2.93544373]\n",
      " [-10.8782696  -10.84635035 -10.85442136 -10.90587378]\n",
      " [-10.26896532 -10.27409697 -10.27291316 -10.32042637]\n",
      " [ -9.60781542  -9.55799349  -9.63402989  -9.59004235]\n",
      " [ -8.81853839  -8.82017489  -8.84835378  -8.89579731]\n",
      " [ -8.1304374   -8.05131396  -8.09017685  -8.07026636]\n",
      " [ -7.29489283  -7.26290799  -7.29988894  -7.26061446]\n",
      " [ -6.51268508  -6.46043211  -6.48268873  -6.54447056]\n",
      " [ -5.65874486  -5.63423247  -5.67468857  -5.75276495]\n",
      " [ -4.80433081  -4.75191416  -4.75107649  -4.88698647]\n",
      " [ -3.93006722  -3.85205179  -3.85436623  -3.93630563]\n",
      " [ -3.09164157  -2.93296532  -2.93407157  -3.01430319]\n",
      " [ -2.01389121  -2.08398942  -1.9890196   -2.1291744 ]\n",
      " [-11.50629595 -11.36151283 -11.68464086 -11.57061459]\n",
      " [-10.69958133 -10.46617457 -20.1779033  -10.7543279 ]\n",
      " [ -9.73299318  -9.5617925  -10.05657592  -9.96442247]\n",
      " [ -8.88484745  -8.64827525 -10.08491631  -8.89403815]\n",
      " [ -7.97245491  -7.72553056 -20.29497545  -7.95617775]\n",
      " [ -7.10065475  -6.79346521 -10.10502417  -7.07970516]\n",
      " [ -6.38991527  -5.85198506 -10.11310703  -6.51139031]\n",
      " [ -5.57553972  -4.90099501 -20.33125735  -5.65702743]\n",
      " [ -3.98811889  -3.940399   -20.34302552  -3.98698366]\n",
      " [ -3.05612457  -2.9701     -10.0583243   -3.16830652]\n",
      " [ -2.10978489  -1.99       -10.01881     -2.33313315]\n",
      " [ -1.26427951  -1.22729016  -1.          -1.14545761]\n",
      " [-12.2478977  -19.         -12.39810239 -12.433237  ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]]\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "biggest_change = 0\n",
    "\n",
    "for episode in range(10_000):\n",
    "    if episode % 50_000 == 0:\n",
    "        print(\"debut de l'épisode:\", episode)\n",
    "    \n",
    "\n",
    "    # debut d'une episode, donc d'un jeu, on initialise des variables :\n",
    "    s, _ = env.reset() # on commence le jeu, et on positionne le taxi dans un état aléatoire \"s\"\n",
    "    \n",
    "    # à partir de l'état \"s\", on choisit une action avec la politique epsilon-greedy :\n",
    "    a = get_greedy_action(s,epsilon) \n",
    "    \n",
    "    # variable qui indique si le jeu est fini ou non :\n",
    "    # au début du jeu, il est sur False, donc pas fini\n",
    "    done = False \n",
    "    \n",
    "    # J'initialise l'état qui suit \"s\" et l'action que je prends à partir de s_, notamment a_ \n",
    "    s_ = None\n",
    "    a_ = None\n",
    "    \n",
    "    while done == False: # tant que le jeu n'est pas fini :\n",
    "        # Je suis dans l'état s, j'applique l'action a, je vais obtenir s_, reward, done,\n",
    "        s_, reward, done, _, info = env.step(a) \n",
    "        \n",
    "        # à partir de s_, on choisit une action a_ avec la méthode epsilon-greedy (epsilon-glutonne)\n",
    "        a_ = get_greedy_action(s_,epsilon)\n",
    "        \n",
    "        # On enregistre la valeur de l'action pour (s,a), Q[s,a], avant et après la modification:\n",
    "        Q[s,a] = Q[s][a]  + alpha*(reward+gamma*Q[s_][a_] - Q[s][a]) # mise à jour avec Sarsa        \n",
    "        # On reinitialise s et a\n",
    "        s = s_ \n",
    "        a = a_ \n",
    "        \n",
    "    iteration += 1\n",
    "\n",
    "print(Q)\n",
    "  \n",
    "print(\"End\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vérification de la politique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -13.0 Done True\n"
     ]
    }
   ],
   "source": [
    "# On vérifie la politique que nous avons apprise, sur un jeu : \n",
    "env = gym.make(game, render_mode=\"human\")\n",
    "G_retour=0.\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done=False\n",
    "while done != True: \n",
    "    env.render()\n",
    "    action = np.argmax(Q[obs])\n",
    "    obs2, rew, done, _, info = env.step(action) #on prend l'action, on passe dans un nouvel état, (case)\n",
    "    G_retour += rew\n",
    "    env.env.s = obs2\n",
    "    obs = obs2  \n",
    "\n",
    "env.render()    \n",
    "print(\"Reward:\", G_retour, \"Done\", done)  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2\n",
      "Step reward: -1\n",
      "Total reward: -8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make(game, render_mode=\"human\")\n",
    "for i in range(10):\n",
    "    G_retour = 0 #recompense \n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        clear_output(wait = True)\n",
    "        print(f'Episode {i+1}')\n",
    "        \n",
    "        env.render()\n",
    "\n",
    "        action = np.argmax(Q[obs])\n",
    "        obs, rew, done, _, info = env.step(action)\n",
    "        G_retour += rew\n",
    "        print(f'Step reward: {rew}')\n",
    "        print(f'Total reward: {G_retour}')\n",
    "        \n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        if done:\n",
    "            print('Episode done')\n",
    "            time.sleep(2)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
